{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Scouting Report Predictability\"\n",
        "format: gfm\n",
        "---\n",
        "\n",
        "\n",
        "# Scouting Report Predictability\n",
        "## How the Words of One NFL Insider Relate to a Player's Chance to Go in the First\n",
        "================\n",
        "## Introduction\n",
        "Professional sports \"insiders\" are paid a lot of money for their insights and analysis on the game they claim to know. But how much of what they say actually holds up in practice? Daniel Jeremiah is a former NFL scot and current NFL Network analyst who posts his ideas on the top 50 prospects in the NFL draft every year. This project will analyze the relationship between his scouting reports and the actual draft results to see how well his words hold up.\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/therealsmithy/nfl-draft-preview/blob/main/vizzes/jeremiah-tweet.png\" width=\"60%\" height=\"60%\" />\n",
        "</p>\n",
        "\n",
        "## Primary Question\n",
        "The main question this project will answer is this: Can a single person's work accurately predict the likelihood of a player going in the first round of the NFL draft? Daniel Jeremiah is the subject of this project due to his status as an NFL Network analyst and the availability of his past scouting reports.\n",
        "\n",
        "## Scraping and Cleaning\n",
        "In order to perform analysis on Jeremiah's statements, they first need to be scraped from the NFL website. For this project his Top 50 prospects from 2020-2025 were scraped and cleaned, then saved into a csv file.\n"
      ],
      "id": "aabf0f4c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Daniel Jeremiah Top 50 2020\n",
        "url = 'https://www.nfl.com/news/daniel-jeremiah-s-top-50-2020-nfl-draft-prospect-rankings-2-0-0ap3000001102767'\n",
        "# Scrape and process\n",
        "response = requests.get(url)\n",
        "soup_20 = BeautifulSoup(response.text, 'html.parser')\n",
        "top_50_2020 = soup_20.select('.nfl-c-article__container')\n",
        "article_range = range(0, len(top_50_2020))\n",
        "top_50_2020 = [top_50_2020[x].text for x in article_range]\n",
        "# Convert to dataframe\n",
        "df_2020 = pd.DataFrame(columns = ['Name', 'Position', 'School', 'Report'])\n",
        "# Pull data into dataframe\n",
        "for i in range(2, 52):\n",
        "    name = re.findall(r'\\d+\\)\\s([A-Za-z\\s\\.\\',\\-]+),', top_50_2020[i])[0]\n",
        "    position = re.findall(r'(?<=, ).*', top_50_2020[i])[0]\n",
        "    school = re.findall(r'(?<=: ).* (?=[|])', top_50_2020[i])[0]\n",
        "    report = max(re.findall(r'(?<=[\\n\\n]).*', top_50_2020[i]), key=len)\n",
        "    df_2020.loc[len(df_2020)] = {'Name': name, 'Position': position, 'School': school, 'Report': report}\n",
        "\n",
        "# Scrape Daniel Jeremiah Top 50 2021\n",
        "url = 'https://www.nfl.com/news/daniel-jeremiah-s-top-50-2021-nfl-draft-prospect-rankings-3-0'\n",
        "# Scrape and process\n",
        "soup_21 = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
        "top_50_2021_info = soup_21.select('.nfl-o-ranked-item__content')\n",
        "top_50_2021_report = soup_21.select('.nfl-c-body-part--text')\n",
        "article_range = range(0, len(top_50_2021))\n",
        "report_range = range(0, len(top_50_2021_report))\n",
        "top_50_2021_info = [top_50_2021[x].text for x in article_range]\n",
        "top_50_2021_report = [top_50_2021_report[x].text for x in report_range]\n",
        "# Convert to dataframe\n",
        "df_2021 = pd.DataFrame(columns = ['Name', 'Position', 'School', 'Report'])\n",
        "# Pull info into dataframe\n",
        "for i in range(0, 50):\n",
        "    name = re.findall(r'([A-Za-z\\s\\.\\',\\-]+)\\n\\n', top_50_2021_info[i])[0]\n",
        "    position = re.findall(r'·\\s*([^·]+)\\s* ·', top_50_2021_info[i])[0]\n",
        "    school = re.findall(r'\\n\\s*([^\\n\\r·]+)\\s*·', top_50_2021_info[i])[0]\n",
        "    df_2021.loc[len(df_2021)] = {'Name': name, 'Position': position, 'School': school, 'Report': top_50_2021_report[i + 1]}\n",
        "# Clean dataframe\n",
        "df_2021['Report'] = df_2021['Report'].str.strip()\n",
        "df_2021['Name'] = df_2021['Name'].str.strip()\n",
        "\n",
        "# Scrape Daniel Jeremiah Top 50 2022\n",
        "url = 'https://www.nfl.com/news/daniel-jeremiah-s-top-50-2022-nfl-draft-prospect-rankings-3-0'\n",
        "# Scrape and process\n",
        "soup_22 = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
        "top_50_2022_info = soup_22.select('.nfl-o-ranked-item__content')\n",
        "top_50_2022_report = soup_22.select('.nfl-c-body-part--text')\n",
        "article_range = range(0, len(top_50_2022_info))\n",
        "report_range = range(0, len(top_50_2022_report))\n",
        "top_50_2022_info = [top_50_2022_info[x].text for x in article_range]\n",
        "top_50_2022_report = [top_50_2022_report[x].text for x in report_range]\n",
        "# Convert to dataframe\n",
        "df_2022 = pd.DataFrame(columns = ['Name', 'Position', 'School', 'Report'])\n",
        "# Pull info into dataframe\n",
        "for i in range(0, 50):\n",
        "    name = re.findall(r'([A-Za-z\\s\\.\\',\\-]+)\\n\\n', top_50_2022_info[i])[0]\n",
        "    position = re.findall(r'·\\s*([^·]+)\\s* ·', top_50_2022_info[i])[0]\n",
        "    school = re.findall(r'\\n\\s*([^\\n\\r·]+)\\s*·', top_50_2022_info[i])[0]\n",
        "    df_2022.loc[len(df_2022)] = {'Name': name, 'Position': position, 'School': school, 'Report': top_50_2022_report[i + 1]}\n",
        "# Clean dataframe\n",
        "df_2022['Report'] = df_2022['Report'].str.strip()\n",
        "df_2022['Name'] = df_2022['Name'].str.strip()\n",
        "\n",
        "# Scrape Daniel Jeremiah Top 50 2023\n",
        "url = 'https://www.nfl.com/news/daniel-jeremiah-s-top-50-2023-nfl-draft-prospect-rankings-4-0'\n",
        "# Scrape and process\n",
        "soup_23 = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
        "top_50_2023_info = soup_23.select('.nfl-o-ranked-item__content')\n",
        "top_50_2023_report = soup_23.select('.nfl-c-body-part--text')\n",
        "article_range = range(0, len(top_50_2023_info))\n",
        "report_range = range(0, len(top_50_2023_report))\n",
        "top_50_2023_info = [top_50_2023_info[x].text for x in article_range]\n",
        "top_50_2023_report = [top_50_2023_report[x].text for x in report_range]\n",
        "# Convert to dataframe\n",
        "df_2023 = pd.DataFrame(columns = ['Name', 'Position', 'School', 'Report'])\n",
        "# Pull info into dataframe\n",
        "for i in range(0, 50):\n",
        "    name = re.findall(r'([A-Za-z\\s\\.\\',\\-]+)\\n\\n', top_50_2023_info[i])[0]\n",
        "    position = re.findall(r'·\\s*([^·]+)\\s* ·', top_50_2023_info[i])[0]\n",
        "    school = re.findall(r'\\n\\s*([^\\n\\r·]+)\\s*·', top_50_2023_info[i])[0]\n",
        "    df_2023.loc[len(df_2023)] = {'Name': name, 'Position': position, 'School': school, 'Report': top_50_2023_report[i + 1]}\n",
        "# Clean dataframe\n",
        "df_2023['Report'] = df_2023['Report'].str.strip()\n",
        "df_2023['Name'] = df_2023['Name'].str.strip()\n",
        "\n",
        "# Scrape Daniel Jeremiah Top 150 2024\n",
        "url = 'https://www.nfl.com/news/daniel-jeremiah-s-top-150-prospects-in-the-2024-nfl-draft-class'\n",
        "# Scrape and process\n",
        "soup_24 = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
        "top_150_2024_info = soup_24.select('.nfl-o-ranked-item__content')\n",
        "top_150_2024_report = soup_24.select('.nfl-c-body-part--text')\n",
        "article_range = range(0, len(top_150_2024_info))\n",
        "report_range = range(0, len(top_150_2024_report))\n",
        "top_150_2024_info = [top_150_2024_info[x].text for x in article_range]\n",
        "top_150_2024_report = [top_150_2024_report[x].text for x in report_range]\n",
        "# Convert to dataframe\n",
        "df_2024 = pd.DataFrame(columns = ['Name', 'Position', 'School', 'Report'])\n",
        "# Pull info into dataframe\n",
        "for i in range(0, 50):\n",
        "    name = re.findall(r'([A-Za-z\\s\\.\\',\\-]+)\\n\\n', top_150_2024_info[i])[0]\n",
        "    position = re.findall(r'·\\s*([^·]+)\\s* ·', top_150_2024_info[i])[0]\n",
        "    school = re.findall(r'\\n\\s*([^\\n\\r·]+)\\s*·', top_150_2024_info[i])[0]\n",
        "    df_2024.loc[len(df_2024)] = {'Name': name, 'Position': position, 'School': school, 'Report': top_150_2024_report[i + 3]}\n",
        "# Clean dataframe\n",
        "df_2024['Report'] = df_2024['Report'].str.strip()\n",
        "df_2024['Name'] = df_2024['Name'].str.strip()\n",
        "\n",
        "# Scrape Daniel Jeremiah Top 50 2025\n",
        "url = 'https://www.nfl.com/news/daniel-jeremiah-s-top-50-2025-nfl-draft-prospect-rankings-1-0'\n",
        "# Scrape and process\n",
        "soup_25 = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
        "top_50_2025_info = soup_25.select('.nfl-o-ranked-item__content')\n",
        "top_50_2025_report = soup_25.select('.nfl-c-body-part--text')\n",
        "article_range = range(0, len(top_50_2025_info))\n",
        "report_range = range(0, len(top_50_2025_report))\n",
        "top_50_2025_info = [top_50_2025_info[x].text for x in article_range]\n",
        "top_50_2025_report = [top_50_2025_report[x].text for x in report_range]\n",
        "# Convert to dataframe\n",
        "df_2025 = pd.DataFrame(columns = ['Name', 'Position', 'School', 'Report'])\n",
        "# Pull info into dataframe\n",
        "for i in range(0, 50):\n",
        "    name = re.findall(r'([A-Za-z\\s\\.\\',\\-]+)\\n\\n', top_50_2025_info[i])[0]\n",
        "    position = re.findall(r'·\\s*([^·]+)\\s* ·', top_50_2025_info[i])[0]\n",
        "    school = re.findall(r'\\n\\s*([^\\n\\r·]+)\\s*·', top_50_2025_info[i])[0]\n",
        "    df_2025.loc[len(df_2025)] = {'Name': name, 'Position': position, 'School': school, 'Report': top_50_2025_report[i + 3]}\n",
        "# Clean dataframe\n",
        "df_2025['Report'] = df_2025['Report'].str.strip()\n",
        "df_2025['Name'] = df_2025['Name'].str.strip()\n",
        "\n",
        "# Save dataframes\n",
        "#df_2020.to_csv('data/scoutingreports/jeremiah_2020.csv', index=False)\n",
        "#df_2021.to_csv('data/scoutingreports/jeremiah_2021.csv', index=False)\n",
        "#df_2022.to_csv('data/scoutingreports/jeremiah_2022.csv', index=False)\n",
        "#df_2023.to_csv('data/scoutingreports/jeremiah_2023.csv', index=False)\n",
        "#df_2024.to_csv('data/scoutingreports/jeremiah_2024.csv', index=False)\n",
        "#df_2025.to_csv('data/scoutingreports/jeremiah_2025.csv', index=False)"
      ],
      "id": "dac48d14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to properly perform text analysis on these reports, the text needs to be properly cleaned and tokenized. The stopwords used to clean the data were scraped from the SMART Stopwords.\n"
      ],
      "id": "573d136b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "# Link to SMART stopwords\n",
        "url = 'http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop'\n",
        "\n",
        "# Get the page and convert it to a BeautifulSoup object\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Convert soup to a list\n",
        "stopwords = soup.get_text().split()\n",
        "\n",
        "# Convert to dataframe and save to CSV\n",
        "stopwords_df = pd.DataFrame(stopwords, columns=['stopword'])\n",
        "#stopwords_df.to_csv('data/stopwords.csv', index=False)"
      ],
      "id": "584d3997",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods\n",
        "Now that the data has been scraped and the stopwords are ready, the next step is to perform text analysis on the reports. The first step is to clean the text and remove stopwords. The next step is to tokenize the text and count the frequency of each word.\n"
      ],
      "id": "6daf7ca8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import dalex as dx\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read in data\n",
        "first_rounders = pd.read_csv('data/2020_to_2024_first_rounders.csv')\n",
        "report_20 = pd.read_csv('data/scoutingreports/jeremiah_2020.csv')\n",
        "report_21 = pd.read_csv('data/scoutingreports/jeremiah_2021.csv')\n",
        "report_22 = pd.read_csv('data/scoutingreports/jeremiah_2022.csv')\n",
        "report_23 = pd.read_csv('data/scoutingreports/jeremiah_2023.csv')\n",
        "report_24 = pd.read_csv('data/scoutingreports/jeremiah_2024.csv')\n",
        "report_25 = pd.read_csv('data/scoutingreports/jeremiah_2025.csv')\n",
        "stopwords = pd.read_csv('data/stopwords.csv')\n",
        "\n",
        "# Combine reports\n",
        "reports_past = pd.concat([report_20, report_21, report_22, report_23, report_24])\n",
        "\n",
        "# Dummy first rounders\n",
        "reports_past['first_rounder'] = reports_past['Name'].isin(first_rounders['Player']).astype(int)\n",
        "\n",
        "# Lemmatize first\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatizer.lemmatize(word) for word in w_tokenizer.tokenize(text)]\n",
        "reports_past['report_lemmatized'] = reports_past['Report'].apply(lemmatize_text)\n",
        "\n",
        "# Remove stopwords\n",
        "reports_past['report_lemmatized'] = reports_past['report_lemmatized'].apply(lambda x: [item for item in x if item not in stopwords['stopword'].values])"
      ],
      "id": "c086f26b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step is to convert the text into a format that can be used in a machine learning model. The TfidfVectorizer will be used to convert the text into a matrix of TF-IDF features.\n"
      ],
      "id": "486a4998"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(xgb_df['report_lemmatized'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "# Make sure output is interpretable\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)"
      ],
      "id": "70634c8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once this is done, the data is ready to be set up for model training. Once this is done we can train the model and see how well it performs initially.\n"
      ],
      "id": "02c53139"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# INITIAL XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', random_state=102701)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_tfidf, xgb_df['first_rounder'], test_size=0.2, random_state=102701)\n",
        "\n",
        "# Fit to training data\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Create basic predictions\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "initial_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Initial accuracy: {initial_accuracy * 100:.2f}%')\n",
        "\n",
        "# Check confusion matrix  to see where the model struggles\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "cm_display = ConfusionMatrixDisplay(conf_matrix).plot()\n",
        "plt.show()"
      ],
      "id": "3911b7aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After this, the model can be tuned to find the best hyperparameters.\n"
      ],
      "id": "fd6e6954"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split validation set from training set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=102701)\n",
        "\n",
        "# Fit model to training data for tuning\n",
        "eval_metrics = ['auc', 'error']\n",
        "model_1 = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 500, \n",
        "                            random_state = 102701, early_stopping_rounds = 10,\n",
        "                            eval_metric=eval_metrics, objective='binary:logistic')\n",
        "eval_set =[(X_train, y_train), (X_val, y_val)]\n",
        "model_1.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
        "\n",
        "# Extract evaluation results\n",
        "eval_results = model_1.evals_result()\n",
        "\n",
        "# Graph error vs. number of iterations\n",
        "plt.figure(figsize = (12, 8))\n",
        "epochs = len(eval_results['validation_1']['error'])\n",
        "x_axis = range(0, epochs)\n",
        "plt.plot(x_axis, eval_results['validation_0']['error'], label = f'Learning Rate: {0.01}')\n",
        "plt.legend()\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('Iterations')\n",
        "plt.title('XGBoost Error vs. Iterations')\n",
        "plt.show()\n",
        "\n",
        "# Set up tuning grid\n",
        "param_grid = {  \n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'min_child_weight': [1, 3, 10],\n",
        "    'learning_rate': [0.1], \n",
        "    'n_estimators': [100], \n",
        "    'subsample': [1.0],\n",
        "    'colsample_bytree': [1.0], \n",
        "    'reg_alpha': [1e-5],\n",
        "    'early_stopping_rounds':[10],\n",
        "    'scale_pos_weight': [0.5626],\n",
        "}\n",
        "\n",
        "# Set up grid search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator = xgb_clf,\n",
        "    param_grid = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    n_jobs = 1,\n",
        "    cv = 5,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
        "\n",
        "# Extract best parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(f'Best parameters: {best_params}')\n",
        "print(f'Best cross-validation score: {best_score}')\n",
        "\n",
        "# Tune subsample and colsample_bytree\n",
        "param_grid = {\n",
        "    'max_depth': [10],\n",
        "    'min_child_weight': [1],\n",
        "    'learning_rate': [0.1], \n",
        "    'n_estimators': [100], \n",
        "    'subsample': [0.5, 0.75, 1.0],\n",
        "    'colsample_bytree': [0.5, 0.75, 1.0], \n",
        "    'reg_alpha': [1e-5],\n",
        "    'early_stopping_rounds':[10],\n",
        "    'scale_pos_weight': [0.5625],\n",
        "}\n",
        "\n",
        "# Set up grid search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator = xgb_clf,\n",
        "    param_grid = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    n_jobs = 1,\n",
        "    cv = 5,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
        "\n",
        "# Extract best parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(f'Best parameters: {best_params}')\n",
        "print(f'Best cross-validation score: {best_score}')\n",
        "\n",
        "# Tune reg_alpha\n",
        "param_grid = {\n",
        "    'max_depth': [10],\n",
        "    'min_child_weight': [1],\n",
        "    'learning_rate': [0.1], \n",
        "    'n_estimators': [100], \n",
        "    'subsample': [1.0],\n",
        "    'colsample_bytree': [0.75], \n",
        "    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n",
        "    'early_stopping_rounds':[10],\n",
        "    'scale_pos_weight': [0.5625],\n",
        "}\n",
        "\n",
        "# Set up grid search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator = xgb_clf,\n",
        "    param_grid = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    n_jobs = 1,\n",
        "    cv = 5,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
        "\n",
        "# Extract best parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(f'Best parameters: {best_params}')\n",
        "print(f'Best cross-validation score: {best_score}')\n",
        "\n",
        "# Tune learning rates\n",
        "learning_rates = [0.0005, 0.001, 0.05, 0.01, 0.03, 0.05, 0.1]\n",
        "eval_results = {}\n",
        "for lr in learning_rates:\n",
        "    model = xgb.XGBClassifier(learning_rate = lr, n_estimators = 500,\n",
        "                            random_state = 102701, early_stopping_rounds = 100,\n",
        "                            eval_metric=eval_metrics, objective='binary:logistic')\n",
        "    eval_set =[(X_train, y_train), (X_val, y_val)]\n",
        "    model.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
        "    eval_results[f'lr_{lr}'] = model.evals_result()\n",
        "\n",
        "# Plot learning rate results\n",
        "plt.figure(figsize = (12, 8))\n",
        "for lr, result in eval_results.items():\n",
        "    epochs = len(result['validation_1']['error'])\n",
        "    x_axis = range(0, epochs)\n",
        "    plt.plot(x_axis, result['validation_1']['error'], label = f'Learning Rate: {lr}')\n",
        "plt.legend()\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('Iterations')\n",
        "plt.title('XGBoost Error vs. Iterations for Varying Learning Rates')\n",
        "plt.show()    \n",
        "\n",
        "# Final tuned model\n",
        "xgb_final = xgb.XGBClassifier(max_depth = 10, min_child_weight = 1, learning_rate = 0.01, \n",
        "                              n_estimators = 100, subsample = 1.0, colsample_bytree = 0.75, \n",
        "                              reg_alpha = 0.1, early_stopping_rounds = 10, \n",
        "                              scale_pos_weight = 0.5625, objective='binary:logistic', \n",
        "                              random_state=102701)\n",
        "\n",
        "# Fit model\n",
        "xgb_final.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)\n",
        "# Predict on test set\n",
        "y_pred = xgb_final.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "id": "48a0b41f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The now tuned model can be used to predict the likelihood of a player going in the first round based on their scouting report for the 2025 class.\n"
      ],
      "id": "18a66e14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply to 2025 prospects\n",
        "report_25['report_lemmatized'] = report_25['Report'].apply(lemmatize_text)\n",
        "report_25['report_lemmatized'] = report_25['report_lemmatized'].apply(lambda x: [item for item in x if item not in stopwords['stopword'].values])\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "tfidf_matrix_25 = vectorizer.transform(report_25['report_lemmatized'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "# Make sure output is interpretable\n",
        "df_tfidf_25 = pd.DataFrame(tfidf_matrix_25.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Predict probabilities on 2025 prospects\n",
        "xgb_preds_proba = xgb_final.predict_proba(df_tfidf_25)[:, 1]\n",
        "\n",
        "# Add predictions to the dataframe\n",
        "report_25['first_rounder_proba'] = xgb_preds_proba\n",
        "\n",
        "# Rank prospects by likelihood of being a first rounder\n",
        "report_25_ranked = report_25.sort_values(by='first_rounder_proba', ascending=False)"
      ],
      "id": "94182504",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "Now that the heavy lifting is done from building the model, the results are ready to be interpreted. The idea here is that this model can be used to predict the likelihood of a player going in the first round based on their scouting report. Perhaps unsurprisingly, the results may not look exactly how we expected them to.\n"
      ],
      "id": "c8133aa6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the ranked prospects\n",
        "print(report_25_ranked[['Name', 'first_rounder_proba']])"
      ],
      "id": "7040a239",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we know how the model feels about the 2025 prospects, it is important to remember that this is just one model and one person's opinion. Fully tuned the model was still only 54% accurate. Despite this, the importance of machine learning interpretability cannot be understated, so let's take a look at the SHAP values to see what the model is thinking.\n"
      ],
      "id": "da84966f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# Get SHAP values\n",
        "explainer = shap.Explainer(xgb_final)\n",
        "shap_values = explainer(df_tfidf_25)\n",
        "\n",
        "# Show top 10 SHAP values\n",
        "shap.summary_plot(shap_values, df_tfidf_25, plot_type=\"bar\", max_display=10)\n",
        "\n",
        "# Plot beeswarm\n",
        "shap.plots.beeswarm(shap_values)\n",
        "\n",
        "# Waterfall for T.J. Sanders\n",
        "shap.plots.waterfall(shap_values[41])\n",
        "\n",
        "# Waterfall for Travis Hunter\n",
        "shap.plots.waterfall(shap_values[1])\n",
        "\n",
        "# Waterfall for Abdul Carter\n",
        "shap.plots.waterfall(shap_values[0])"
      ],
      "id": "7b36f6ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/therealsmithy/nfl-draft-preview/blob/main/vizzes/report-beeswarm-v1.jpeg\" width=\"60%\" height=\"60%\" />\n",
        "  <figcaption align=\"center\">Beeswarm Visualization of SHAP values</figcaption>\n",
        "</p>\n",
        "\n",
        "The model assigns high values of \"starter\", \"move\", and \"overall\" with a negative impact on the likelihood of being a first rounder. These words may be tough to interpret in this scenario. Moving down, higher values of slot is also associated with lower first-round likelihood. A \"slot\" reciever is a player who lines up in the slot position, which is between the offensive line and the outside receiver. This position is typically not as highly valued as the outside receiver position, which may explain the negative impact on the likelihood of being a first rounder. As expected, higher values of \"size\", \"tackle\", and \"quickness\" are associated with a higher likelihood of being a first rounder. These are all attributes that are highly valued in the NFL.\n",
        "\n",
        "Next, let's take a look at some interesting player cases and see why the model attributed the probability that it did to them. First up, T.J. Sanders.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/therealsmithy/nfl-draft-preview/blob/main/vizzes/sanders-waterfall-v1.jpeg\" width=\"60%\" height=\"60%\" />\n",
        "  <figcaption align=\"center\">Waterfall Visualization of Important Features influencing T.J. Sanders</figcaption>\n",
        "</p>\n",
        "\n",
        "As previously stated, the words \"starter\" and \"move\" negatively impact a player's likelihood of being a first rounder. In Sanders' case, these words do not appear in his scouting report, leading to them greatly impacting his first-round likelihood. The words \"blocks\", \"explosive\", and \"tackle\" all appear in his report with a positive impact.\n",
        "\n",
        "The next player to inspect is Travis Hunter.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/therealsmithy/nfl-draft-preview/blob/main/vizzes/hunter-waterfall-v1.jpeg\" width=\"60%\" height=\"60%\" />\n",
        "  <figcaption align=\"center\">Waterfall Visualization of Important Features influencing Travis Hunter</figcaption>\n",
        "</p>\n",
        "\n",
        "Just like Sanders, Hunter does not have the words \"starter\" or \"move\" in his report. For anyone who has kept up with college football discourse in 2024, one of Hunter's greatest attributes is his ability to play both sides of the ball. This is reflected in the word \"time\" having a positive value and one of the highest impacts on his first-round likelihood.\n",
        "\n",
        "Finally, let's take a look at Abdul Carter. Carter is widely considered as one of the best prospects in this class, but the model has him very low on the first-round likelihood list. Let's inspect why.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/therealsmithy/nfl-draft-preview/blob/main/vizzes/carter-waterfall-v1.jpeg\" width=\"60%\" height=\"60%\" />\n",
        "  <figcaption align=\"center\">Waterfall Visualization of Important Features influencing Abdul Carter</figcaption>\n",
        "</p>\n",
        "\n",
        "When looking at this graph, the high negative impact of the word \"move\" jumps off the screen. Carter's ability to move between positions on the defense, whether on the line or as a linebacker, is a key part of his game. The model interprets \"move\" as a negative attribute, which is likely why Carter is so low on the first-round likelihood list.\n",
        "\n",
        "## Discussion\n",
        "At the end of the day, Insiders like Jeremiah are paid for their ability to generate buzz and \"clicks\". As far as I know they do not voice their opinions on players so that people can run Machine Learning models on them but that is exactly what I did. The low model accuracy and the negative impact of certain words on a player's likelihood of being a first rounder show that the model may not be the best way to predict a player's draft position. However, the SHAP values show that the model is able to pick up on certain attributes that are highly valued in the NFL, such as size and quickness. The main takeaway from this project is that while scouting reports may be interesting to read and generate buzz, they may not be the best way to predict a player's draft position."
      ],
      "id": "7c958608"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\liams\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}